{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data and train labels\n",
    "X_train = pd.read_csv('data/EstrogenReceptorStatus_Train.csv',index_col=0)\n",
    "y_train = pd.read_csv('data/EstrogenReceptorStatus_Train_labels.txt',header=None)\n",
    "\n",
    "# convert them to numpy arrays\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data and test labels\n",
    "X_test = pd.read_csv('data/EstrogenReceptorStatus_Test.csv',index_col=0)\n",
    "y_test = pd.read_csv('data/EstrogenReceptorStatus_Test_labels.txt',header=None)\n",
    "\n",
    "# convert them to numpy arrays\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 54 55\n"
     ]
    }
   ],
   "source": [
    "# split training data and labels into training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=14)\n",
    "# check training = 60%, validation = 20% and test = 20%\n",
    "print(len(y_train), len(y_val), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features (data) to tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "# convert labels to tensors\n",
    "y_train = torch.LongTensor(y_train)[:,0]\n",
    "y_val = torch.FloatTensor(y_val)[:,0]\n",
    "y_test = torch.LongTensor(y_test)[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "n_features = 162\n",
    "\n",
    "n_examples_train = len(y_train)\n",
    "n_examples_test = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(weighted_sum):\n",
    "    most_probable = weighted_sum.argmax().item()\n",
    "    output = 0 if most_probable < 0 else 1\n",
    "    output = torch.tensor(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Neural Network, it is a FeedForward\n",
    "class Network(nn.Module):\n",
    "    # create the layers of the Network: input layer, two hidden layers, output layer.\n",
    "    def __init__(self, in_features=n_features, h1=30, h2=30, out_features=2):\n",
    "        super().__init__() # instantiate the model\n",
    "        torch.manual_seed(14)\n",
    "        self.fc1 = nn.Linear(in_features, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.out = nn.Linear(h2, out_features)\n",
    "\n",
    "    # set the activations functions that will be used in every layer\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.out(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "\n",
    "# set the criterion model to measure the loss/error\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "# set the optimizer and learning rate\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "losses = []\n",
    "stop = (n_examples_train//batch_size)*batch_size\n",
    "\n",
    "def predict_part(a, b):\n",
    "    # go for a prediction\n",
    "    y_pred = network.forward(X_train[a:b,:])\n",
    "    # measure the loss/error\n",
    "    loss = loss_criterion(y_pred, y_train[a:b])\n",
    "    # keep track of the losses\n",
    "    losses.append(loss.detach().numpy()) # we dont want it to save it as a tensor\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# function for the test/evaluation in which we dont want back propagation, it takes into acount mini batches\n",
    "# variable that counts how many correct predictions we've got in the evaluation\n",
    "correct = 0\n",
    "def evaluation(a,b):\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(X_test[a:b,]):\n",
    "            y_eval = network.forward(data)\n",
    "            # get the number of correct predicitions\n",
    "            ## print(y_eval.argmax().item(),y_test[i].item())\n",
    "            if y_eval.argmax().item() == y_test[i].item(): \n",
    "                global correct\n",
    "                correct += 1\n",
    "\n",
    "# in this function we get the indexes for the batches                \n",
    "def get_batches(n_examples, train=1): # train is used to decide if the we are on training or in testing\n",
    "    for begin in range(0, n_examples, batch_size):\n",
    "        # indexes for all the batches but the last one\n",
    "        if begin != stop:\n",
    "            # define the index for the last example that will be taken into acount in the current batch\n",
    "            final = begin+batch_size\n",
    "            # decide if it is training or testing\n",
    "            if train: predict_part(begin, final)\n",
    "            else: evaluation(begin, final)\n",
    "        # indexes fot the last batch\n",
    "        else: \n",
    "            # decide if it is training or testing\n",
    "            if train: predict_part(begin, X_train.shape[0])\n",
    "            else: evaluation(begin, X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\COMPUTOCKS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8669473528862 0.31326165795326233\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    get_batches(n_examples_train)\n",
    "print(losses[0].item(), losses[-1].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "Accuracy: 0.8909090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\COMPUTOCKS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# test the network\n",
    "get_batches(n_examples_test, 0)\n",
    "print(f'Accuracy: {correct/len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# weight and biases run\\nimport wandb\\n# start a new wandb run to track this script\\nwandb.init(\\n    # set the wandb project where this run will be logged\\n    project=\"EstrogenReceptor_FeedForwardNN\",\\n    \\n    # track hyperparameters and run metadata\\n    config={\\n    \"learning_rate\": 0.02,\\n    \"architecture\": \"CNN\",\\n    \"dataset\": \"CIFAR-100\",\\n    \"epochs\": 10,\\n    }\\n)\\n\\nlosses = []\\nfor epoch in range(epochs):\\n    for batch in range(0, )\\n    # go for a prediction\\n    y_pred = network.forward(X_train)\\n    # measure the loss/error\\n    loss = loss_criterion(y_pred, y_train)\\n    # back propagation\\n    loss.backward()\\n    optimizer.step()\\n    optimizer.zero_grad()    \\n# [optional] finish the wandb run, necessary in notebooks\\nwandb.finish()\\n'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# weight and biases run\n",
    "import wandb\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"EstrogenReceptor_FeedForwardNN\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, )\n",
    "    # go for a prediction\n",
    "    y_pred = network.forward(X_train)\n",
    "    # measure the loss/error\n",
    "    loss = loss_criterion(y_pred, y_train)\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
